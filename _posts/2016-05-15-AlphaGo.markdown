---
layout: post
title:  AlphaGo
date:   2016-05-15 14:54:15
---

- 拜师学艺

![](/image/learn.jpeg)

 围棋每一步平均有200种可能的走法，这只是数学的理论值，指每一步合乎规则的走法，然而合乎规则并不代表合理。事实证明很多走法都是无理手，是无用的，必输的，人类顶级棋手永远不会走。所以每一步合理的走法，平均根本没有200种，也许平均只有20种。为此，AlphaGo从类似QQ游戏大厅的网上围棋对战平台[KGS](http://www.gokgs.com/)获得了10万局高段位棋手对弈的棋谱，然后打造一个“[深度卷积神经网络](https://en.wikipedia.org/wiki/Convolutional_neural_network)”，让AlphaGo学习人类棋手怎么走。最终AlphaGo在走每一步之前，只会锁定人类围棋高手最常落子的位置，而忽略其他位置，至此“深度卷积神经网络”让AlphaGo具备学习的能力。


- 十年寒窗

![](/image/practice.jpg)

 AlphaGo已经通过学习人类下棋的方式，取得了降低计算质量方面质的突破。接下来，AlphaGo采用了一种“[蒙特卡洛搜索树](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search)”的算法，类似于两个人通过丢骰子的方式来决定每一步该如何落子，这样最后总会有一个人是赢的。然后把赢的那一局里所有的落子方案都提高一点权重，然后这样这些方案下次被丢骰子选中的几率就会高一些。随着这样的棋局数量越来越多，那些经常赢的落子方案，权重会越来越高，被选中的几率也越来越高，最后它们会被当作好的落子方案浮现出来。

- 左右互搏

![](/image/fight.jpg)

 AlphaG再把前面的“深度卷积神经网络”和“蒙特卡洛搜索树”算法结合使用后，围棋水平大幅提升，但与人类职业选手依然有差距。为此，AlphaGo构建另一套“深度卷积神经网络”，通过程序自己和自己对弈的方式，专门训练AlphaGo对局面的判断。最终AlphaGo对局面的判断达到非常准确的水平，从而达到了可以挑战人类围棋冠军的实力。


参考文献

 - Silver D, Huang A, Maddison C J, et al. Mastering the game of Go with deep neural networks and tree search[J]. Nature, 2016, 529(7587): 484-489.
